apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kas-monitor-alerts
  namespace: openshift-route-monitor-operator
spec:
  groups:
  - name: kas-monitor-rules
    rules:
    - alert: kas-monitor-ErrorBudgetBurn
      expr: |
        1 - (sum by (probe_url, namespace, _id, cluster) (sum_over_time(probe_success{}[5m])) / sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[5m]))) > (14.4 * (1 - 0.9995)) and sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[5m])) > 5 and 1 - (sum by (probe_url, namespace, _id, cluster) (sum_over_time(probe_success{}[1h])) / sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[1h]))) > (14.4 * (1 - 0.9995)) and sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[1h])) > 60
      for: 2m
      labels:
        long_window: 1h
        severity: warning
        short_window: 5m
      annotations:
        description: "High error budget burn for {{ $labels.probe_url }} (current value: {{ $value }})"
    - alert: kas-monitor-ErrorBudgetBurn
      expr: 1 - (sum by (probe_url, namespace, _id, cluster) (sum_over_time(probe_success{}[30m])) / sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[30m]))) > (6 * (1 - 0.9995)) and sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[30m])) > 30 and 1 - (sum by (probe_url, namespace, _id, cluster) (sum_over_time(probe_success{}[6h])) / sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[6h]))) > (6 * (1 - 0.9995)) and sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[6h])) > 360
      for: 15m
      labels:
        long_window: 6h
        severity: warning
        short_window: 30m
      annotations:
        description: "High error budget burn for {{ $labels.probe_url }} (current value: {{ $value }})"
    - alert: kas-monitor-ErrorBudgetBurn
      expr: 1 - (sum by (probe_url, namespace, _id, cluster) (sum_over_time(probe_success{}[2h])) / sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[2h]))) > (3 * (1 - 0.9995)) and sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[2h])) > 120 and 1 - (sum by (probe_url, namespace, _id, cluster) (sum_over_time(probe_success{}[1d])) / sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[1d]))) > (3 * (1 - 0.9995)) and sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[1d])) > 1440
      for: 1h
      labels:
        long_window: 1d
        severity: info
        short_window: 2h
      annotations:
        description: "High error budget burn for {{ $labels.probe_url }} (current value: {{ $value }})"
    - alert: kas-monitor-ErrorBudgetBurn
      expr: 1 - (sum by (probe_url, namespace, _id, cluster) (sum_over_time(probe_success{}[6h])) / sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[6h]))) > (1 * (1 - 0.9995)) and sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[6h])) > 360 and 1 - (sum by (probe_url, namespace, _id, cluster) (sum_over_time(probe_success{}[3d])) / sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[3d]))) > (1 * (1 - 0.9995)) and sum by (probe_url, namespace, _id, cluster) (count_over_time(probe_success{}[3d])) > 4320
      for: 3h
      labels:
        long_window: 3d
        severity: info
        short_window: 6h
      annotations:
        description: "High error budget burn for {{ $labels.probe_url }} (current value: {{ $value }})"
    # ServiceMonitor Creation Monitoring - Detects when HCP namespaces exist but probe_success metrics are missing
    # This indicates RMO, Blackbox Exporter, or Prometheus failures preventing monitoring coverage
    # SLO: 99% of HCPs should have monitoring (1% error budget, 14.4x burn rate = 14.4% threshold)
    - alert: kas-monitor-ServiceMonitorCreationErrorBudgetBurn
      expr: |
        (
          (
            count(kube_namespace_status_phase{phase="Active", namespace=~"ocm-aro.*-.*-.*"})
            -
            count(probe_success{namespace=~"ocm-aro.*"})
          )
          /
          count(kube_namespace_status_phase{phase="Active", namespace=~"ocm-aro.*-.*-.*"})
        ) > 0.144
      for: 15m
      labels:
        severity: warning
        component: route-monitor-operator
        slo: hcp-monitoring-coverage
      annotations:
        summary: "HCP KAS monitoring coverage below SLO"
        description: "{{ $value | humanizePercentage }} of HCPs are missing probe_success metrics. SLO threshold: 14.4%. This indicates RMO failed to create ServiceMonitor, Blackbox Exporter is not probing, or Prometheus is not scraping."
        runbook_url: TBD

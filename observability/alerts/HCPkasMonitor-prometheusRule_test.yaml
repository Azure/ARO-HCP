rule_files:
- HCPkasMonitor-prometheusRule.yaml
evaluation_interval: 1m
tests:
# Test 1: Normal operation - perfect success, no alerts should fire
- interval: 1m
  input_series:
  - series: 'probe_success{probe_url="https://api.example.com/health", namespace="test-hcp-namespace", _id="test-hcp-id", cluster="test-cluster"}'
    values: "1+0x70" # Perfect success for sufficient time
  alert_rule_test:
  - eval_time: 65m
    alertname: kas-monitor-ErrorBudgetBurn
    exp_alerts: [] # Should not fire with perfect success
# Test 2: Complete failure should trigger fast burn alert (30m/1h windows, for: 2m)
- interval: 30s # Generate data every 30 seconds to meet sample count requirements
  input_series:
  - series: 'probe_success{probe_url="https://api.example.com/health", namespace="test-hcp-namespace", _id="test-hcp-id", cluster="test-cluster"}'
    # Complete failure: 0% success (100% failure) for sufficient duration
    # With 30s interval: 30m window has 60 samples (>5 ✓), 1h window has 120 samples (>60 ✓)
    # Alert condition: 1 - (0/60) > 0.0072 AND 1 - (0/120) > 0.0072 = TRUE
    # With 'for: 2m', alert should fire at 63m (condition true at 61m + 2m)
    values: "0+0x130" # Complete failure for 65+ minutes (130 * 30s = 65m)
  alert_rule_test:
  - eval_time: 63m # When first alert should fire (after 'for' condition satisfied)
    alertname: kas-monitor-ErrorBudgetBurn
    exp_alerts:
    - exp_labels:
        severity: warning
        long_window: 1h
        short_window: 30m
        probe_url: https://api.example.com/health
        namespace: test-hcp-namespace
        _id: test-hcp-id
        cluster: test-cluster
      exp_annotations:
        summary: "High error budget burn for https://api.example.com/health"
        description: "High error budget burn for https://api.example.com/health (current value: 1)"
# Test 3: Medium burn rate - Alert 2 (30m/6h windows, for: 15m)
- interval: 30s # Generate data every 30 seconds
  input_series:
  - series: 'probe_success{probe_url="https://api.example.com/health", namespace="test-hcp-namespace", _id="test-hcp-id", cluster="test-cluster"}'
    # Complete failure for sufficient duration
    # With 30s interval: 30m window has 60 samples (>30 ✓), 6h window has 720 samples (>360 ✓)
    values: "0+0x800" # Complete failure for 400+ minutes (800 * 30s = 400m)
  alert_rule_test:
  - eval_time: 390m # After 6h+30m to satisfy both windows and 'for' condition (15m)
    alertname: kas-monitor-ErrorBudgetBurn
    exp_alerts:
    - exp_labels:
        severity: warning
        long_window: 1h
        short_window: 30m
        probe_url: https://api.example.com/health
        namespace: test-hcp-namespace
        _id: test-hcp-id
        cluster: test-cluster
      exp_annotations:
        summary: "High error budget burn for https://api.example.com/health"
        description: "High error budget burn for https://api.example.com/health (current value: 1)"
    - exp_labels:
        severity: warning
        long_window: 6h
        short_window: 30m
        probe_url: https://api.example.com/health
        namespace: test-hcp-namespace
        _id: test-hcp-id
        cluster: test-cluster
      exp_annotations:
        summary: "High error budget burn for https://api.example.com/health"
        description: "High error budget burn for https://api.example.com/health (current value: 1)"
# Test 4: All four alerts should fire simultaneously after prolonged failure
- interval: 30s
  input_series:
  - series: 'probe_success{probe_url="https://api.example.com/health", namespace="test-hcp-namespace", _id="test-hcp-id", cluster="test-cluster"}'
    # Complete failure for 3+ days to satisfy all four alert time windows
    values: "0+0x9000" # 4500 minutes = 75 hours = 3+ days of failure
  alert_rule_test:
  - eval_time: 4500m # After sufficient time for all windows + 'for' conditions
    alertname: kas-monitor-ErrorBudgetBurn
    exp_alerts:
    - exp_labels:
        severity: warning
        long_window: 1h
        short_window: 30m
        probe_url: https://api.example.com/health
        namespace: test-hcp-namespace
        _id: test-hcp-id
        cluster: test-cluster
      exp_annotations:
        summary: "High error budget burn for https://api.example.com/health"
        description: "High error budget burn for https://api.example.com/health (current value: 1)"
    - exp_labels:
        severity: warning
        long_window: 6h
        short_window: 30m
        probe_url: https://api.example.com/health
        namespace: test-hcp-namespace
        _id: test-hcp-id
        cluster: test-cluster
      exp_annotations:
        summary: "High error budget burn for https://api.example.com/health"
        description: "High error budget burn for https://api.example.com/health (current value: 1)"
    - exp_labels:
        severity: info
        long_window: 1d
        short_window: 2h
        probe_url: https://api.example.com/health
        namespace: test-hcp-namespace
        _id: test-hcp-id
        cluster: test-cluster
      exp_annotations:
        summary: "High error budget burn for https://api.example.com/health"
        description: "High error budget burn for https://api.example.com/health (current value: 1)"
    - exp_labels:
        severity: info
        long_window: 3d
        short_window: 6h
        probe_url: https://api.example.com/health
        namespace: test-hcp-namespace
        _id: test-hcp-id
        cluster: test-cluster
      exp_annotations:
        summary: "High error budget burn for https://api.example.com/health"
        description: "High error budget burn for https://api.example.com/health (current value: 1)"
# NEW TESTS FOR kas-monitor-ServiceMonitorCreationErrorBudgetBurn
# Note: In production, each HCP has 2 namespaces:
#   - Management NS: ocm-aro{env}-{id} (2 segments) - NOT counted by pattern
#   - HCP NS: ocm-aro{env}-{id}-{cluster} (3+ segments) - COUNTED by pattern ocm-aro.*-.*-.*
# Uses kube_namespace_status_phase (current state) instead of kube_namespace_created
# Test 5: Below threshold - no alert (10% missing = below 14.4%)
- interval: 1m
  name: "Test 5: ServiceMonitor coverage 90% - below threshold, no alert"
  input_series:
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster1"}'
    values: '1+0x10'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster2"}'
    values: '1+0x10'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster3"}'
    values: '1+0x10'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster4"}'
    values: '1+0x10'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster5"}'
    values: '1+0x10'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster6"}'
    values: '1+0x10'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster7"}'
    values: '1+0x10'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster8"}'
    values: '1+0x10'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster9"}'
    values: '1+0x10'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster10"}'
    values: '1+0x10'
  # 9 out of 10 have probe_success (10% missing)
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster1"}'
    values: '1+0x10'
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster2"}'
    values: '1+0x10'
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster3"}'
    values: '1+0x10'
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster4"}'
    values: '1+0x10'
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster5"}'
    values: '1+0x10'
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster6"}'
    values: '1+0x10'
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster7"}'
    values: '1+0x10'
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster8"}'
    values: '1+0x10'
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster9"}'
    values: '1+0x10'
  alert_rule_test:
  - eval_time: 10m
    alertname: kas-monitor-ServiceMonitorCreationErrorBudgetBurn
    exp_alerts: []
# Test 6: Above threshold - alert fires (20% missing = above 14.4%)
- interval: 1m
  name: "Test 6: ServiceMonitor coverage 80% - above threshold, alert fires"
  input_series:
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster1"}'
    values: '1+0x20'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster2"}'
    values: '1+0x20'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster3"}'
    values: '1+0x20'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster4"}'
    values: '1+0x20'
  - series: 'kube_namespace_status_phase{phase="Active", namespace="ocm-arohcpint-xxx-cluster5"}'
    values: '1+0x20'
  # 4 out of 5 have probe_success (20% missing)
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster1"}'
    values: '1+0x20'
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster2"}'
    values: '1+0x20'
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster3"}'
    values: '1+0x20'
  - series: 'probe_success{namespace="ocm-arohcpint-xxx-cluster4"}'
    values: '1+0x20'
  alert_rule_test:
  - eval_time: 14m
    alertname: kas-monitor-ServiceMonitorCreationErrorBudgetBurn
    exp_alerts: [] # Still pending (for: 15m not satisfied yet)
  - eval_time: 16m
    alertname: kas-monitor-ServiceMonitorCreationErrorBudgetBurn
    exp_alerts:
    - exp_labels:
        severity: warning
        component: route-monitor-operator
        slo: hcp-monitoring-coverage
      exp_annotations:
        summary: "HCP KAS monitoring coverage below SLO"
        description: "20% of HCPs are missing probe_success metrics. SLO threshold: 14.4%. This indicates RMO failed to create ServiceMonitor, Blackbox Exporter is not probing, or Prometheus is not scraping."
        runbook_url: TBD

kube-prometheus-stack:
  fullnameOverride: "prometheus"
  crds:
    enabled: true
  global:
    rbac:
      create: true
  alertmanager:
    enabled: false # Disabled for now.
  grafana:
    enabled: false
  # Azure Managed Prometheus will scrape or expose these metrics
  coreDns:
    enabled: false
  kubeDns:
    enabled: false
  nodeExporter:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeControllerManager:
    enabled: false
  kubeProxy:
    enabled: false
  kubelet:
    enabled: false
  kubeApiServer:
    enabled: false
  kubeStateMetrics:
    enabled: true # OSS prometheus scrapes kube-state-metrics so the HCP ksm are routed to the hcp Azure Monitor Workspace
  prometheus:
    enabled: false # Prometheus is deployed using templates/prometheus.yaml
  prometheusOperator:
    enabled: true
    fullnameOverride: ""
    image:
      registry: "{{ .mgmt.prometheus.prometheusOperator.image.registry }}"
      repository: "{{ .mgmt.prometheus.prometheusOperator.image.repository }}"
      sha: "{{ .mgmt.prometheus.prometheusOperator.image.sha }}"
    prometheusConfigReloader:
      image:
        registry: "{{ .mgmt.prometheus.prometheusConfigReloader.image.registry }}"
        repository: "{{ .mgmt.prometheus.prometheusConfigReloader.image.repository }}"
        sha: "{{ .mgmt.prometheus.prometheusConfigReloader.image.sha }}"
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: aro-hcp.azure.com/role
              operator: In
              values:
              - "infra"
    tolerations:
    - key: "infra"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
    admissionWebhooks:
      patch:
        enabled: true
        image:
          registry: "{{ .acr.svc.name }}.{{ .acrDNSSuffix }}"
          repository: "{{ .mgmt.prometheus.admissionWebhook.patch.image.repository }}"
          sha: "{{ .mgmt.prometheus.admissionWebhook.patch.image.sha }}"
  ## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.
  cleanPrometheusOperatorObjectNames: true
  # config for kube-state-metrics subchart that is imported by the kube-prometheus-stack chart conditionally
  kube-state-metrics:
    image:
      registry: "{{ .mgmt.prometheus.kubeStateMetrics.image.registry }}"
      repository: "{{ .mgmt.prometheus.kubeStateMetrics.image.repository }}"
      sha: "{{ .mgmt.prometheus.kubeStateMetrics.image.digest }}"
# this is our own config for the files in templates/
prometheusSpec:
  image:
    registry: "{{ .mgmt.prometheus.prometheusSpec.image.registry }}"
    repository: "{{ .mgmt.prometheus.prometheusSpec.image.repository }}"
    sha: "{{ .mgmt.prometheus.prometheusSpec.image.sha }}"
  version: "" # derived from the sha
  externalLabels:
    cluster: "{{ .mgmt.aks.name }}"
  remoteWriteUrl: "__dcrRemoteWriteUrl__"
  hcpRemoteWriteUrl: "__hcpDcrRemoteWriteUrl__"
  zoneCount: {{ .azureRegionAvailabilityZoneCount }}
  maximumStartupDurationSeconds: 360
  retention: 1d
  retentionSize: 45GiB
  resources:
    requests:
      cpu: {{ .mgmt.prometheus.prometheusSpec.resources.requests.cpu }}
      memory: {{ .mgmt.prometheus.prometheusSpec.resources.requests.memory }}
    limits:
      cpu: {{ .mgmt.prometheus.prometheusSpec.resources.limits.cpu }}
      memory: {{ .mgmt.prometheus.prometheusSpec.resources.limits.memory }}
prometheus:
  prometheusSpec:
    shards: {{ .mgmt.prometheus.prometheusSpec.shards }}
    replicas: {{ .mgmt.prometheus.prometheusSpec.replicas }}
  serviceAccount:
    managedIdentity: "__prometheusUAMIClientId__"
environment: "{{ .clustersService.environment }}"
